# RAG Chatbot with Ollama

A Retrieval-Augmented Generation (RAG) chatbot built with LangChain and Ollama, featuring both Streamlit and Flask interfaces.

## Features

- ğŸ¤– RAG-based question answering
- ğŸ¨ Modern Flask web interface
- ğŸ“± Streamlit chat interface
- ğŸ” FAISS vector search
- ğŸ¦™ Ollama integration

## Setup Instructions

### 1. Install Ollama and Models
```bash
ollama pull gemma3:latest
ollama pull nomic-embed-text
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Create Vector Database
```bash
python setup_vectorstore.py
```

### 4. Run the Application

**Flask Web Interface (Recommended):**
```bash
python app_flask.py
```
Then open http://127.0.0.1:5000

**Streamlit Interface:**
```bash
streamlit run app.py
```

## Project Structure

```
Wikipediarag/
â”œâ”€â”€ app.py              # Streamlit interface
â”œâ”€â”€ app_flask.py        # Flask web interface
â”œâ”€â”€ setup_vectorstore.py # Vector database setup
â”œâ”€â”€ dataset.txt         # Your dataset
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      # Flask HTML template
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md          # This file
```

## How it Works

1. **Data Processing**: Loads `dataset.txt` and splits into chunks
2. **Embeddings**: Creates embeddings using `nomic-embed-text` model
3. **Vector Store**: Stores embeddings in FAISS for fast similarity search
4. **Question Answering**: Uses `gemma3:latest` model to generate answers based on retrieved context

## Technologies Used

- **LangChain**: Framework for LLM applications
- **Ollama**: Local LLM inference
- **FAISS**: Vector similarity search
- **Flask**: Web framework
- **Streamlit**: Rapid prototyping interface
